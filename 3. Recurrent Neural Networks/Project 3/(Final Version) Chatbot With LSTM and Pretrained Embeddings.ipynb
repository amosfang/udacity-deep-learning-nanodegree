{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import gzip\n",
    "import torch\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of size of SQuAD data\n",
    "\n",
    "DATA_SIZE = 5000\n",
    "\n",
    "# Choice of model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import SQuAD2\n",
    "from torch.utils.data.datapipes.iter import IterableWrapper\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def loadDF(path):\n",
    "    '''\n",
    "  \n",
    "      You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "    '''\n",
    "    data_pipe = SQuAD2(path, split='train')\n",
    "    \n",
    "    # Convert DataPipe to an iterable\n",
    "    iterable_data = IterableWrapper(data_pipe)\n",
    "    \n",
    "    # Convert DataPipe to dictionary with lists of question and answers\n",
    "    data_dict = {\n",
    "        'Question': [],\n",
    "        'Answers': []\n",
    "    }\n",
    "    \n",
    "    for _, question, answers, _ in iterable_data:\n",
    "        data_dict['Question'].append(question)\n",
    "        data_dict['Answers'].append(answers)\n",
    "        \n",
    "    \n",
    "    # Convert dictionary to pandas dataframe\n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Our text needs to be cleaned with a tokenizer. This function will perform that task.\n",
    "    https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "    '''\n",
    "    if isinstance(sentence, list):\n",
    "\n",
    "        # Answers are contained in string within a list\n",
    "        tokens = wordpunct_tokenize(sentence[0].lower())\n",
    "    else:\n",
    "        tokens = wordpunct_tokenize(sentence.lower())\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build your Vocabulary & create the Word Embeddings\n",
    "\n",
    "* The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "\n",
    "(Extra Credit)\n",
    "\n",
    "* Use Gensim to extract the word embeddings from one of its corpus'.\n",
    "* Use NLTK and Gensim to create a function to clean your text and look up the index of a word's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = loadDF('data')\n",
    "\n",
    "data_df = data_df.iloc[:DATA_SIZE, :]\n",
    "\n",
    "# Remove questions that did not have answers\n",
    "empty_list = ['']\n",
    "data_df = data_df = data_df[data_df.Answers.apply(lambda x: x != empty_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Question'] = data_df['Question'].apply(prepare_text)\n",
    "data_df['Answers'] = data_df['Answers'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, did, beyonce, start, becoming, popular, ?]</td>\n",
       "      <td>[in, the, late, 1990s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what, areas, did, beyonce, compete, in, when,...</td>\n",
       "      <td>[singing, and, dancing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[when, did, beyonce, leave, destiny, ', s, chi...</td>\n",
       "      <td>[2003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[in, what, city, and, state, did, beyonce, gro...</td>\n",
       "      <td>[houston, ,, texas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[in, which, decade, did, beyonce, become, famo...</td>\n",
       "      <td>[late, 1990s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[in, what, r, &amp;, b, group, was, she, the, lead...</td>\n",
       "      <td>[destiny, ', s, child]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[what, album, made, her, a, worldwide, known, ...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[who, managed, the, destiny, ', s, child, grou...</td>\n",
       "      <td>[mathew, knowles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[when, did, beyoncé, rise, to, fame, ?]</td>\n",
       "      <td>[late, 1990s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[what, role, did, beyoncé, have, in, destiny, ...</td>\n",
       "      <td>[lead, singer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[what, was, the, first, album, beyoncé, releas...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[when, did, beyoncé, release, dangerously, in,...</td>\n",
       "      <td>[2003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[how, many, grammy, awards, did, beyoncé, win,...</td>\n",
       "      <td>[five]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[what, was, beyoncé, ', s, role, in, destiny, ...</td>\n",
       "      <td>[lead, singer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[what, was, the, name, of, beyoncé, ', s, firs...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[after, her, second, solo, album, ,, what, oth...</td>\n",
       "      <td>[acting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[which, artist, did, beyonce, marry, ?]</td>\n",
       "      <td>[jay, z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[to, set, the, record, for, grammys, ,, how, m...</td>\n",
       "      <td>[six]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[for, what, movie, did, beyonce, receive, her,...</td>\n",
       "      <td>[dreamgirls]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[when, did, beyonce, take, a, hiatus, in, her,...</td>\n",
       "      <td>[2010]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question                  Answers\n",
       "0   [when, did, beyonce, start, becoming, popular, ?]   [in, the, late, 1990s]\n",
       "1   [what, areas, did, beyonce, compete, in, when,...  [singing, and, dancing]\n",
       "2   [when, did, beyonce, leave, destiny, ', s, chi...                   [2003]\n",
       "3   [in, what, city, and, state, did, beyonce, gro...      [houston, ,, texas]\n",
       "4   [in, which, decade, did, beyonce, become, famo...            [late, 1990s]\n",
       "5   [in, what, r, &, b, group, was, she, the, lead...   [destiny, ', s, child]\n",
       "6   [what, album, made, her, a, worldwide, known, ...  [dangerously, in, love]\n",
       "7   [who, managed, the, destiny, ', s, child, grou...        [mathew, knowles]\n",
       "8             [when, did, beyoncé, rise, to, fame, ?]            [late, 1990s]\n",
       "9   [what, role, did, beyoncé, have, in, destiny, ...           [lead, singer]\n",
       "10  [what, was, the, first, album, beyoncé, releas...  [dangerously, in, love]\n",
       "11  [when, did, beyoncé, release, dangerously, in,...                   [2003]\n",
       "12  [how, many, grammy, awards, did, beyoncé, win,...                   [five]\n",
       "13  [what, was, beyoncé, ', s, role, in, destiny, ...           [lead, singer]\n",
       "14  [what, was, the, name, of, beyoncé, ', s, firs...  [dangerously, in, love]\n",
       "15  [after, her, second, solo, album, ,, what, oth...                 [acting]\n",
       "16            [which, artist, did, beyonce, marry, ?]                 [jay, z]\n",
       "17  [to, set, the, record, for, grammys, ,, how, m...                    [six]\n",
       "18  [for, what, movie, did, beyonce, receive, her,...             [dreamgirls]\n",
       "19  [when, did, beyonce, take, a, hiatus, in, her,...                   [2010]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all lists in the 'Question' column\n",
    "Q_vocab_list = sum(data_df['Question'], [])\n",
    "A_vocab_list = sum(data_df['Answers'], [])\n",
    "# Return list of unique words\n",
    "Q_vocab_list = list(set(Q_vocab_list))\n",
    "A_vocab_list = list(set(A_vocab_list))\n",
    "# Add the <EOS> token\n",
    "A_vocab_list.append('<EOS>')\n",
    "\n",
    "Q_vocab_list.sort()\n",
    "A_vocab_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_word_embeddings(vocab_list, w2v_index_to_key_list, w2v_wv_vectors):\n",
    "    key_list = []\n",
    "    new_embeddings = []\n",
    "    \n",
    "    # generic word embeddings for missing word\n",
    "    # picked a random word \"\"\n",
    "    rand_embedding = w2v_wv_vectors[5225]\n",
    "\n",
    "    embedding_dim = w2v_wv_vectors.shape[1]\n",
    "\n",
    "    # if word is found in the brown corpus, then use its embeddings, \n",
    "    # else randomly assign new word embedding\n",
    "    for word in vocab_list:\n",
    "        try:\n",
    "            key_list.append(w2v_index_to_key_list.index(word))\n",
    "        except:\n",
    "            key_list.append(-9999)\n",
    "\n",
    "    for key in key_list:\n",
    "        if key != -9999:\n",
    "            new_embeddings.append(w2v_wv_vectors[key])\n",
    "        else:\n",
    "            new_embeddings.append(rand_embedding)\n",
    "\n",
    "    # convert a list of np.arrays to a stack of np.arrays\n",
    "    new_embeddings = np.vstack(new_embeddings)\n",
    "    \n",
    "    return new_embeddings\n",
    "\n",
    "Q_embeddings = prepare_word_embeddings(Q_vocab_list, w2v.wv.index_to_key, w2v.wv.vectors)\n",
    "A_embeddings = prepare_word_embeddings(A_vocab_list, w2v.wv.index_to_key, w2v.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, model, vocab_list):\n",
    "        self.model = model\n",
    "        \n",
    "        self.words = vocab_list\n",
    "        # contains dictionary with key, value (index, word)\n",
    "        self.index2word = dict(enumerate(self.words))\n",
    "        # contains dictionary with key, value (word, index)\n",
    "        self.word2index = {word: index for index, word in self.index2word.items()}\n",
    "\n",
    "    def indexWord(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vocab = Vocab(w2v, Q_vocab_list)\n",
    "A_vocab = Vocab(w2v, A_vocab_list)\n",
    "\n",
    "# Define the EOS TOKEN index as global variable\n",
    "EOS_INDEX = A_vocab_list.index('<EOS>')\n",
    "\n",
    "def encode_Q_sentence(words):\n",
    "    output = [Q_vocab.indexWord(word) for word in words]\n",
    "    return output\n",
    "\n",
    "def encode_A_sentence(words):\n",
    "    output = [A_vocab.indexWord(word) for word in words]\n",
    "    output.append(EOS_INDEX)\n",
    "    return output\n",
    "\n",
    "data_df['Question_encoded'] = data_df['Question'].apply(encode_Q_sentence)\n",
    "data_df['Answers_encoded'] = data_df['Answers'].apply(encode_A_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Question_encoded</th>\n",
       "      <th>Answers_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, did, beyonce, start, becoming, popular, ?]</td>\n",
       "      <td>[in, the, late, 1990s]</td>\n",
       "      <td>[5296, 1498, 699, 4631, 659, 3690, 222]</td>\n",
       "      <td>[1973, 3597, 2216, 223, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what, areas, did, beyonce, compete, in, when,...</td>\n",
       "      <td>[singing, and, dancing]</td>\n",
       "      <td>[5293, 500, 1498, 699, 1130, 2510, 5296, 4408,...</td>\n",
       "      <td>[3356, 531, 1151, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[when, did, beyonce, leave, destiny, ', s, chi...</td>\n",
       "      <td>[2003]</td>\n",
       "      <td>[5296, 1498, 699, 2853, 1469, 5, 4246, 967, 43...</td>\n",
       "      <td>[236, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[in, what, city, and, state, did, beyonce, gro...</td>\n",
       "      <td>[houston, ,, texas]</td>\n",
       "      <td>[2510, 5293, 1015, 438, 4637, 1498, 699, 2253,...</td>\n",
       "      <td>[1919, 11, 3591, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[in, which, decade, did, beyonce, become, famo...</td>\n",
       "      <td>[late, 1990s]</td>\n",
       "      <td>[2510, 5299, 1391, 1498, 699, 657, 1901, 222]</td>\n",
       "      <td>[2216, 223, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[in, what, r, &amp;, b, group, was, she, the, lead...</td>\n",
       "      <td>[destiny, ', s, child]</td>\n",
       "      <td>[2510, 5293, 3917, 4, 601, 2250, 5253, 4408, 4...</td>\n",
       "      <td>[1210, 4, 3202, 927, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[what, album, made, her, a, worldwide, known, ...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "      <td>[5293, 381, 2981, 2352, 225, 5359, 2771, 518, ...</td>\n",
       "      <td>[1153, 1973, 2299, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[who, managed, the, destiny, ', s, child, grou...</td>\n",
       "      <td>[mathew, knowles]</td>\n",
       "      <td>[5303, 3013, 4887, 1469, 5, 4246, 967, 2250, 222]</td>\n",
       "      <td>[2383, 2172, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[when, did, beyoncé, rise, to, fame, ?]</td>\n",
       "      <td>[late, 1990s]</td>\n",
       "      <td>[5296, 1498, 700, 4179, 4954, 1896, 222]</td>\n",
       "      <td>[2216, 223, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[what, role, did, beyoncé, have, in, destiny, ...</td>\n",
       "      <td>[lead, singer]</td>\n",
       "      <td>[5293, 4201, 1498, 700, 2312, 2510, 1469, 5, 4...</td>\n",
       "      <td>[2223, 3355, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[what, was, the, first, album, beyoncé, releas...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "      <td>[5293, 5253, 4887, 1992, 381, 700, 4064, 522, ...</td>\n",
       "      <td>[1153, 1973, 2299, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[when, did, beyoncé, release, dangerously, in,...</td>\n",
       "      <td>[2003]</td>\n",
       "      <td>[5296, 1498, 700, 4063, 1362, 2510, 2956, 222]</td>\n",
       "      <td>[236, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[how, many, grammy, awards, did, beyoncé, win,...</td>\n",
       "      <td>[five]</td>\n",
       "      <td>[2441, 3029, 2224, 597, 1498, 700, 5323, 2025,...</td>\n",
       "      <td>[1551, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[what, was, beyoncé, ', s, role, in, destiny, ...</td>\n",
       "      <td>[lead, singer]</td>\n",
       "      <td>[5293, 5253, 700, 5, 4246, 4201, 2510, 1469, 5...</td>\n",
       "      <td>[2223, 3355, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[what, was, the, name, of, beyoncé, ', s, firs...</td>\n",
       "      <td>[dangerously, in, love]</td>\n",
       "      <td>[5293, 5253, 4887, 3241, 3374, 700, 5, 4246, 1...</td>\n",
       "      <td>[1153, 1973, 2299, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[after, her, second, solo, album, ,, what, oth...</td>\n",
       "      <td>[acting]</td>\n",
       "      <td>[344, 2352, 4334, 4531, 381, 10, 5293, 3448, 1...</td>\n",
       "      <td>[440, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[which, artist, did, beyonce, marry, ?]</td>\n",
       "      <td>[jay, z]</td>\n",
       "      <td>[5299, 518, 1498, 699, 3048, 222]</td>\n",
       "      <td>[2084, 3983, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[to, set, the, record, for, grammys, ,, how, m...</td>\n",
       "      <td>[six]</td>\n",
       "      <td>[4954, 4383, 4887, 4006, 2025, 2225, 10, 2441,...</td>\n",
       "      <td>[3363, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[for, what, movie, did, beyonce, receive, her,...</td>\n",
       "      <td>[dreamgirls]</td>\n",
       "      <td>[2025, 5293, 3206, 1498, 699, 3983, 2352, 1992...</td>\n",
       "      <td>[1291, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[when, did, beyonce, take, a, hiatus, in, her,...</td>\n",
       "      <td>[2010]</td>\n",
       "      <td>[5296, 1498, 699, 4806, 225, 2359, 2510, 2352,...</td>\n",
       "      <td>[244, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[which, album, was, darker, in, tone, from, he...</td>\n",
       "      <td>[beyoncé]</td>\n",
       "      <td>[5299, 381, 5253, 1366, 2510, 4963, 2086, 2352...</td>\n",
       "      <td>[709, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[after, what, movie, portraying, etta, james, ...</td>\n",
       "      <td>[cadillac, records]</td>\n",
       "      <td>[344, 5293, 3206, 3705, 1814, 2676, 10, 1498, ...</td>\n",
       "      <td>[827, 3055, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[when, did, destiny, ', s, child, end, their, ...</td>\n",
       "      <td>[june, 2005]</td>\n",
       "      <td>[5296, 1498, 1469, 5, 4246, 967, 1734, 4891, 2...</td>\n",
       "      <td>[2119, 238, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[what, was, the, name, of, beyoncé, ', s, seco...</td>\n",
       "      <td>[b, ', day]</td>\n",
       "      <td>[5293, 5253, 4887, 3241, 3374, 700, 5, 4246, 4...</td>\n",
       "      <td>[638, 4, 1167, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[what, was, beyoncé, ', s, first, acting, job,...</td>\n",
       "      <td>[dreamgirls]</td>\n",
       "      <td>[5293, 5253, 700, 5, 4246, 1992, 285, 2700, 10...</td>\n",
       "      <td>[1291, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[who, is, beyoncé, married, to, ?]</td>\n",
       "      <td>[jay, z]</td>\n",
       "      <td>[5303, 2651, 700, 3047, 4954, 222]</td>\n",
       "      <td>[2084, 3983, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[what, is, the, name, of, beyoncé, ', s, alter...</td>\n",
       "      <td>[sasha, fierce]</td>\n",
       "      <td>[5293, 2651, 4887, 3241, 3374, 700, 5, 4246, 4...</td>\n",
       "      <td>[3224, 1526, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[in, her, music, ,, what, are, some, recurring...</td>\n",
       "      <td>[love, ,, relationships, ,, and, monogamy]</td>\n",
       "      <td>[2510, 2352, 3225, 10, 5293, 498, 4535, 4016, ...</td>\n",
       "      <td>[2299, 11, 3076, 11, 531, 2506, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[time, magazine, named, her, one, of, the, mos...</td>\n",
       "      <td>[influential]</td>\n",
       "      <td>[4945, 2986, 3242, 2352, 3398, 3374, 4887, 319...</td>\n",
       "      <td>[1996, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[which, magazine, declared, her, the, most, do...</td>\n",
       "      <td>[forbes]</td>\n",
       "      <td>[5299, 2986, 1400, 2352, 4887, 3192, 1582, 534...</td>\n",
       "      <td>[1581, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[in, which, decade, did, the, recording, indus...</td>\n",
       "      <td>[2000s]</td>\n",
       "      <td>[2510, 5299, 1391, 1498, 4887, 4008, 2543, 544...</td>\n",
       "      <td>[233, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[what, magazine, rated, beyonce, as, the, most...</td>\n",
       "      <td>[forbes]</td>\n",
       "      <td>[5293, 2986, 3955, 699, 522, 4887, 3192, 3724,...</td>\n",
       "      <td>[1581, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[how, did, beyonce, describe, herself, as, a, ...</td>\n",
       "      <td>[modern, -, day, feminist]</td>\n",
       "      <td>[2441, 1498, 699, 1455, 2355, 522, 225, 1945, ...</td>\n",
       "      <td>[2493, 12, 1167, 1512, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[in, which, years, did, time, rate, beyonce, i...</td>\n",
       "      <td>[2013, and, 2014]</td>\n",
       "      <td>[2510, 5299, 5387, 1498, 4945, 3954, 699, 2510...</td>\n",
       "      <td>[247, 531, 248, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[how, many, records, has, beyonce, sold, in, h...</td>\n",
       "      <td>[118, million]</td>\n",
       "      <td>[2441, 3029, 4010, 2311, 699, 4526, 2510, 2352...</td>\n",
       "      <td>[43, 2457, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[how, many, records, did, beyoncé, sell, as, p...</td>\n",
       "      <td>[60, million]</td>\n",
       "      <td>[2441, 3029, 4010, 1498, 700, 4358, 522, 3518,...</td>\n",
       "      <td>[351, 2457, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[after, leaving, destiny, ', s, child, ,, how,...</td>\n",
       "      <td>[118, million]</td>\n",
       "      <td>[344, 2854, 1469, 5, 4246, 967, 10, 2441, 3029...</td>\n",
       "      <td>[43, 2457, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[how, many, grammy, awards, has, beyoncé, won, ?]</td>\n",
       "      <td>[20]</td>\n",
       "      <td>[2441, 3029, 2224, 597, 2311, 700, 5344, 222]</td>\n",
       "      <td>[230, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[what, magazine, named, beyoncé, as, the, most...</td>\n",
       "      <td>[forbes]</td>\n",
       "      <td>[5293, 2986, 3242, 700, 522, 4887, 3192, 3724,...</td>\n",
       "      <td>[1581, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[beyonce, ', s, younger, sibling, also, sang, ...</td>\n",
       "      <td>[destiny, ', s, child]</td>\n",
       "      <td>[699, 5, 4246, 5404, 4448, 405, 4272, 5335, 23...</td>\n",
       "      <td>[1210, 4, 3202, 927, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[where, did, beyonce, get, her, name, from, ?]</td>\n",
       "      <td>[her, mother, ', s, maiden, name]</td>\n",
       "      <td>[5297, 1498, 699, 2158, 2352, 3241, 2086, 222]</td>\n",
       "      <td>[1853, 2520, 4, 3202, 2337, 2557, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[what, race, was, beyonce, ', s, father, ?]</td>\n",
       "      <td>[african, -, american]</td>\n",
       "      <td>[5293, 3920, 5253, 699, 5, 4246, 1914, 222]</td>\n",
       "      <td>[458, 12, 519, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[beyonce, ', s, childhood, home, believed, in,...</td>\n",
       "      <td>[methodist]</td>\n",
       "      <td>[699, 5, 4246, 968, 2402, 680, 2510, 5293, 406...</td>\n",
       "      <td>[2427, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[beyonce, ', s, father, worked, as, a, sales, ...</td>\n",
       "      <td>[xerox]</td>\n",
       "      <td>[699, 5, 4246, 1914, 5351, 522, 225, 4257, 301...</td>\n",
       "      <td>[3952, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[beyonce, ', s, mother, worked, in, what, indu...</td>\n",
       "      <td>[hairdresser, and, salon, owner]</td>\n",
       "      <td>[699, 5, 4246, 3194, 5351, 2510, 5293, 2543, 222]</td>\n",
       "      <td>[1793, 531, 3210, 2738, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[what, younger, sister, of, beyonce, also, app...</td>\n",
       "      <td>[solange]</td>\n",
       "      <td>[5293, 5404, 4480, 3374, 699, 405, 478, 2510, ...</td>\n",
       "      <td>[3381, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[beyonce, is, a, descendent, of, what, arcadia...</td>\n",
       "      <td>[joseph, broussard]</td>\n",
       "      <td>[699, 2651, 225, 1454, 3374, 5293, 493, 2839, ...</td>\n",
       "      <td>[2109, 795, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[what, company, did, beyoncé, ', s, father, wo...</td>\n",
       "      <td>[xerox]</td>\n",
       "      <td>[5293, 1118, 1498, 700, 5, 4246, 1914, 5350, 2...</td>\n",
       "      <td>[3952, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[what, did, beyoncé, ', s, mother, own, when, ...</td>\n",
       "      <td>[salon]</td>\n",
       "      <td>[5293, 1498, 700, 5, 4246, 3194, 3470, 5296, 7...</td>\n",
       "      <td>[3210, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[what, is, the, name, of, beyoncé, ', s, young...</td>\n",
       "      <td>[solange]</td>\n",
       "      <td>[5293, 2651, 4887, 3241, 3374, 700, 5, 4246, 5...</td>\n",
       "      <td>[3381, 417]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0   [when, did, beyonce, start, becoming, popular, ?]   \n",
       "1   [what, areas, did, beyonce, compete, in, when,...   \n",
       "2   [when, did, beyonce, leave, destiny, ', s, chi...   \n",
       "3   [in, what, city, and, state, did, beyonce, gro...   \n",
       "4   [in, which, decade, did, beyonce, become, famo...   \n",
       "5   [in, what, r, &, b, group, was, she, the, lead...   \n",
       "6   [what, album, made, her, a, worldwide, known, ...   \n",
       "7   [who, managed, the, destiny, ', s, child, grou...   \n",
       "8             [when, did, beyoncé, rise, to, fame, ?]   \n",
       "9   [what, role, did, beyoncé, have, in, destiny, ...   \n",
       "10  [what, was, the, first, album, beyoncé, releas...   \n",
       "11  [when, did, beyoncé, release, dangerously, in,...   \n",
       "12  [how, many, grammy, awards, did, beyoncé, win,...   \n",
       "13  [what, was, beyoncé, ', s, role, in, destiny, ...   \n",
       "14  [what, was, the, name, of, beyoncé, ', s, firs...   \n",
       "15  [after, her, second, solo, album, ,, what, oth...   \n",
       "16            [which, artist, did, beyonce, marry, ?]   \n",
       "17  [to, set, the, record, for, grammys, ,, how, m...   \n",
       "18  [for, what, movie, did, beyonce, receive, her,...   \n",
       "19  [when, did, beyonce, take, a, hiatus, in, her,...   \n",
       "20  [which, album, was, darker, in, tone, from, he...   \n",
       "21  [after, what, movie, portraying, etta, james, ...   \n",
       "22  [when, did, destiny, ', s, child, end, their, ...   \n",
       "23  [what, was, the, name, of, beyoncé, ', s, seco...   \n",
       "24  [what, was, beyoncé, ', s, first, acting, job,...   \n",
       "25                 [who, is, beyoncé, married, to, ?]   \n",
       "26  [what, is, the, name, of, beyoncé, ', s, alter...   \n",
       "27  [in, her, music, ,, what, are, some, recurring...   \n",
       "28  [time, magazine, named, her, one, of, the, mos...   \n",
       "29  [which, magazine, declared, her, the, most, do...   \n",
       "30  [in, which, decade, did, the, recording, indus...   \n",
       "31  [what, magazine, rated, beyonce, as, the, most...   \n",
       "32  [how, did, beyonce, describe, herself, as, a, ...   \n",
       "33  [in, which, years, did, time, rate, beyonce, i...   \n",
       "34  [how, many, records, has, beyonce, sold, in, h...   \n",
       "35  [how, many, records, did, beyoncé, sell, as, p...   \n",
       "36  [after, leaving, destiny, ', s, child, ,, how,...   \n",
       "37  [how, many, grammy, awards, has, beyoncé, won, ?]   \n",
       "38  [what, magazine, named, beyoncé, as, the, most...   \n",
       "39  [beyonce, ', s, younger, sibling, also, sang, ...   \n",
       "40     [where, did, beyonce, get, her, name, from, ?]   \n",
       "41        [what, race, was, beyonce, ', s, father, ?]   \n",
       "42  [beyonce, ', s, childhood, home, believed, in,...   \n",
       "43  [beyonce, ', s, father, worked, as, a, sales, ...   \n",
       "44  [beyonce, ', s, mother, worked, in, what, indu...   \n",
       "45  [what, younger, sister, of, beyonce, also, app...   \n",
       "46  [beyonce, is, a, descendent, of, what, arcadia...   \n",
       "47  [what, company, did, beyoncé, ', s, father, wo...   \n",
       "48  [what, did, beyoncé, ', s, mother, own, when, ...   \n",
       "49  [what, is, the, name, of, beyoncé, ', s, young...   \n",
       "\n",
       "                                       Answers  \\\n",
       "0                       [in, the, late, 1990s]   \n",
       "1                      [singing, and, dancing]   \n",
       "2                                       [2003]   \n",
       "3                          [houston, ,, texas]   \n",
       "4                                [late, 1990s]   \n",
       "5                       [destiny, ', s, child]   \n",
       "6                      [dangerously, in, love]   \n",
       "7                            [mathew, knowles]   \n",
       "8                                [late, 1990s]   \n",
       "9                               [lead, singer]   \n",
       "10                     [dangerously, in, love]   \n",
       "11                                      [2003]   \n",
       "12                                      [five]   \n",
       "13                              [lead, singer]   \n",
       "14                     [dangerously, in, love]   \n",
       "15                                    [acting]   \n",
       "16                                    [jay, z]   \n",
       "17                                       [six]   \n",
       "18                                [dreamgirls]   \n",
       "19                                      [2010]   \n",
       "20                                   [beyoncé]   \n",
       "21                         [cadillac, records]   \n",
       "22                                [june, 2005]   \n",
       "23                                 [b, ', day]   \n",
       "24                                [dreamgirls]   \n",
       "25                                    [jay, z]   \n",
       "26                             [sasha, fierce]   \n",
       "27  [love, ,, relationships, ,, and, monogamy]   \n",
       "28                               [influential]   \n",
       "29                                    [forbes]   \n",
       "30                                     [2000s]   \n",
       "31                                    [forbes]   \n",
       "32                  [modern, -, day, feminist]   \n",
       "33                           [2013, and, 2014]   \n",
       "34                              [118, million]   \n",
       "35                               [60, million]   \n",
       "36                              [118, million]   \n",
       "37                                        [20]   \n",
       "38                                    [forbes]   \n",
       "39                      [destiny, ', s, child]   \n",
       "40           [her, mother, ', s, maiden, name]   \n",
       "41                      [african, -, american]   \n",
       "42                                 [methodist]   \n",
       "43                                     [xerox]   \n",
       "44            [hairdresser, and, salon, owner]   \n",
       "45                                   [solange]   \n",
       "46                         [joseph, broussard]   \n",
       "47                                     [xerox]   \n",
       "48                                     [salon]   \n",
       "49                                   [solange]   \n",
       "\n",
       "                                     Question_encoded  \\\n",
       "0             [5296, 1498, 699, 4631, 659, 3690, 222]   \n",
       "1   [5293, 500, 1498, 699, 1130, 2510, 5296, 4408,...   \n",
       "2   [5296, 1498, 699, 2853, 1469, 5, 4246, 967, 43...   \n",
       "3   [2510, 5293, 1015, 438, 4637, 1498, 699, 2253,...   \n",
       "4       [2510, 5299, 1391, 1498, 699, 657, 1901, 222]   \n",
       "5   [2510, 5293, 3917, 4, 601, 2250, 5253, 4408, 4...   \n",
       "6   [5293, 381, 2981, 2352, 225, 5359, 2771, 518, ...   \n",
       "7   [5303, 3013, 4887, 1469, 5, 4246, 967, 2250, 222]   \n",
       "8            [5296, 1498, 700, 4179, 4954, 1896, 222]   \n",
       "9   [5293, 4201, 1498, 700, 2312, 2510, 1469, 5, 4...   \n",
       "10  [5293, 5253, 4887, 1992, 381, 700, 4064, 522, ...   \n",
       "11     [5296, 1498, 700, 4063, 1362, 2510, 2956, 222]   \n",
       "12  [2441, 3029, 2224, 597, 1498, 700, 5323, 2025,...   \n",
       "13  [5293, 5253, 700, 5, 4246, 4201, 2510, 1469, 5...   \n",
       "14  [5293, 5253, 4887, 3241, 3374, 700, 5, 4246, 1...   \n",
       "15  [344, 2352, 4334, 4531, 381, 10, 5293, 3448, 1...   \n",
       "16                  [5299, 518, 1498, 699, 3048, 222]   \n",
       "17  [4954, 4383, 4887, 4006, 2025, 2225, 10, 2441,...   \n",
       "18  [2025, 5293, 3206, 1498, 699, 3983, 2352, 1992...   \n",
       "19  [5296, 1498, 699, 4806, 225, 2359, 2510, 2352,...   \n",
       "20  [5299, 381, 5253, 1366, 2510, 4963, 2086, 2352...   \n",
       "21  [344, 5293, 3206, 3705, 1814, 2676, 10, 1498, ...   \n",
       "22  [5296, 1498, 1469, 5, 4246, 967, 1734, 4891, 2...   \n",
       "23  [5293, 5253, 4887, 3241, 3374, 700, 5, 4246, 4...   \n",
       "24  [5293, 5253, 700, 5, 4246, 1992, 285, 2700, 10...   \n",
       "25                 [5303, 2651, 700, 3047, 4954, 222]   \n",
       "26  [5293, 2651, 4887, 3241, 3374, 700, 5, 4246, 4...   \n",
       "27  [2510, 2352, 3225, 10, 5293, 498, 4535, 4016, ...   \n",
       "28  [4945, 2986, 3242, 2352, 3398, 3374, 4887, 319...   \n",
       "29  [5299, 2986, 1400, 2352, 4887, 3192, 1582, 534...   \n",
       "30  [2510, 5299, 1391, 1498, 4887, 4008, 2543, 544...   \n",
       "31  [5293, 2986, 3955, 699, 522, 4887, 3192, 3724,...   \n",
       "32  [2441, 1498, 699, 1455, 2355, 522, 225, 1945, ...   \n",
       "33  [2510, 5299, 5387, 1498, 4945, 3954, 699, 2510...   \n",
       "34  [2441, 3029, 4010, 2311, 699, 4526, 2510, 2352...   \n",
       "35  [2441, 3029, 4010, 1498, 700, 4358, 522, 3518,...   \n",
       "36  [344, 2854, 1469, 5, 4246, 967, 10, 2441, 3029...   \n",
       "37      [2441, 3029, 2224, 597, 2311, 700, 5344, 222]   \n",
       "38  [5293, 2986, 3242, 700, 522, 4887, 3192, 3724,...   \n",
       "39  [699, 5, 4246, 5404, 4448, 405, 4272, 5335, 23...   \n",
       "40     [5297, 1498, 699, 2158, 2352, 3241, 2086, 222]   \n",
       "41        [5293, 3920, 5253, 699, 5, 4246, 1914, 222]   \n",
       "42  [699, 5, 4246, 968, 2402, 680, 2510, 5293, 406...   \n",
       "43  [699, 5, 4246, 1914, 5351, 522, 225, 4257, 301...   \n",
       "44  [699, 5, 4246, 3194, 5351, 2510, 5293, 2543, 222]   \n",
       "45  [5293, 5404, 4480, 3374, 699, 405, 478, 2510, ...   \n",
       "46  [699, 2651, 225, 1454, 3374, 5293, 493, 2839, ...   \n",
       "47  [5293, 1118, 1498, 700, 5, 4246, 1914, 5350, 2...   \n",
       "48  [5293, 1498, 700, 5, 4246, 3194, 3470, 5296, 7...   \n",
       "49  [5293, 2651, 4887, 3241, 3374, 700, 5, 4246, 5...   \n",
       "\n",
       "                           Answers_encoded  \n",
       "0             [1973, 3597, 2216, 223, 417]  \n",
       "1                   [3356, 531, 1151, 417]  \n",
       "2                               [236, 417]  \n",
       "3                    [1919, 11, 3591, 417]  \n",
       "4                         [2216, 223, 417]  \n",
       "5                [1210, 4, 3202, 927, 417]  \n",
       "6                  [1153, 1973, 2299, 417]  \n",
       "7                        [2383, 2172, 417]  \n",
       "8                         [2216, 223, 417]  \n",
       "9                        [2223, 3355, 417]  \n",
       "10                 [1153, 1973, 2299, 417]  \n",
       "11                              [236, 417]  \n",
       "12                             [1551, 417]  \n",
       "13                       [2223, 3355, 417]  \n",
       "14                 [1153, 1973, 2299, 417]  \n",
       "15                              [440, 417]  \n",
       "16                       [2084, 3983, 417]  \n",
       "17                             [3363, 417]  \n",
       "18                             [1291, 417]  \n",
       "19                              [244, 417]  \n",
       "20                              [709, 417]  \n",
       "21                        [827, 3055, 417]  \n",
       "22                        [2119, 238, 417]  \n",
       "23                     [638, 4, 1167, 417]  \n",
       "24                             [1291, 417]  \n",
       "25                       [2084, 3983, 417]  \n",
       "26                       [3224, 1526, 417]  \n",
       "27    [2299, 11, 3076, 11, 531, 2506, 417]  \n",
       "28                             [1996, 417]  \n",
       "29                             [1581, 417]  \n",
       "30                              [233, 417]  \n",
       "31                             [1581, 417]  \n",
       "32             [2493, 12, 1167, 1512, 417]  \n",
       "33                    [247, 531, 248, 417]  \n",
       "34                         [43, 2457, 417]  \n",
       "35                        [351, 2457, 417]  \n",
       "36                         [43, 2457, 417]  \n",
       "37                              [230, 417]  \n",
       "38                             [1581, 417]  \n",
       "39               [1210, 4, 3202, 927, 417]  \n",
       "40  [1853, 2520, 4, 3202, 2337, 2557, 417]  \n",
       "41                     [458, 12, 519, 417]  \n",
       "42                             [2427, 417]  \n",
       "43                             [3952, 417]  \n",
       "44            [1793, 531, 3210, 2738, 417]  \n",
       "45                             [3381, 417]  \n",
       "46                        [2109, 795, 417]  \n",
       "47                             [3952, 417]  \n",
       "48                             [3210, 417]  \n",
       "49                             [3381, 417]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Question_encoded</th>\n",
       "      <th>Answers_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>[what, was, the, name, of, west, ', s, fashion...</td>\n",
       "      <td>[dw, kanye, west]</td>\n",
       "      <td>[5293, 5253, 4887, 3241, 3374, 5291, 5, 4246, ...</td>\n",
       "      <td>[1314, 2128, 3880, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>[the, fashion, line, shown, in, paris, receive...</td>\n",
       "      <td>[mixed, -, to, -, negative]</td>\n",
       "      <td>[4887, 1910, 2904, 4440, 2510, 3508, 3984, 529...</td>\n",
       "      <td>[2485, 12, 3640, 12, 2581, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>[on, what, day, did, west, release, his, secon...</td>\n",
       "      <td>[march, 6, ,, 2012]</td>\n",
       "      <td>[3396, 5293, 1372, 1498, 5291, 4063, 2381, 433...</td>\n",
       "      <td>[2363, 350, 11, 246, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>[what, brand, struck, a, deal, with, kanye, an...</td>\n",
       "      <td>[adidas]</td>\n",
       "      <td>[5293, 773, 4696, 225, 1382, 5335, 2738, 438, ...</td>\n",
       "      <td>[449, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>[how, many, \", seasons, \", of, clothing, did, ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2441, 3029, 0, 4332, 0, 3374, 1047, 1498, 273...</td>\n",
       "      <td>[280, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>[what, were, the, shoes, designed, by, kanye, ...</td>\n",
       "      <td>[adidas, yeezy, boosts]</td>\n",
       "      <td>[5293, 5290, 4887, 4425, 1463, 829, 2738, 438,...</td>\n",
       "      <td>[449, 3971, 751, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>[how, many, pairs, of, shoes, were, sold, in, ...</td>\n",
       "      <td>[9000]</td>\n",
       "      <td>[2441, 3029, 3490, 3374, 4425, 5290, 4526, 251...</td>\n",
       "      <td>[402, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>[what, shoe, was, announced, on, twitter, by, ...</td>\n",
       "      <td>[adidas, yeezy, boosts]</td>\n",
       "      <td>[5293, 4424, 5253, 451, 3396, 5069, 829, 2738,...</td>\n",
       "      <td>[449, 3971, 751, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>[in, what, year, did, kanye, premier, his, sea...</td>\n",
       "      <td>[2015]</td>\n",
       "      <td>[2510, 5293, 5385, 1498, 2738, 3748, 2381, 433...</td>\n",
       "      <td>[249, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>[what, album, release, coincided, with, kanye,...</td>\n",
       "      <td>[the, life, of, pablo]</td>\n",
       "      <td>[5293, 381, 4063, 1059, 5335, 2738, 5, 4246, 5...</td>\n",
       "      <td>[3597, 2253, 2673, 2741, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>[what, restaurant, chain, did, kanye, aspire, ...</td>\n",
       "      <td>[fatburger]</td>\n",
       "      <td>[5293, 4137, 923, 1498, 2738, 530, 4954, 3402,...</td>\n",
       "      <td>[1493, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>[how, many, restaurants, did, kanye, open, ?]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2441, 3029, 4138, 1498, 2738, 3402, 222]</td>\n",
       "      <td>[229, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>[what, happened, to, the, fatburger, chains, t...</td>\n",
       "      <td>[shut, down]</td>\n",
       "      <td>[5293, 2298, 4954, 4887, 1913, 924, 4886, 2738...</td>\n",
       "      <td>[3342, 1286, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>[what, was, the, name, of, the, restaurant, ka...</td>\n",
       "      <td>[fatburger]</td>\n",
       "      <td>[5293, 5253, 4887, 3241, 3374, 4887, 4137, 273...</td>\n",
       "      <td>[1493, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>[how, many, of, kanye, ', s, fatburger, restau...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2441, 3029, 3374, 2738, 5, 4246, 1913, 4138, ...</td>\n",
       "      <td>[229, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>[when, was, kanye, ', s, last, fatburger, rest...</td>\n",
       "      <td>[2011]</td>\n",
       "      <td>[5296, 5253, 2738, 5, 4246, 2820, 1913, 4137, ...</td>\n",
       "      <td>[245, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>[what, is, the, name, of, kanye, west, ', s, f...</td>\n",
       "      <td>[kw, foods, llc]</td>\n",
       "      <td>[5293, 2651, 4887, 3241, 3374, 2738, 5291, 5, ...</td>\n",
       "      <td>[2190, 1575, 2281, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>[what, did, kanye, call, the, label, he, found...</td>\n",
       "      <td>[good, music]</td>\n",
       "      <td>[5293, 1498, 2738, 837, 4887, 2785, 2316, 2055...</td>\n",
       "      <td>[1739, 2544, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>[as, of, 2015, ,, who, did, kanye, choose, as,...</td>\n",
       "      <td>[pusha, t]</td>\n",
       "      <td>[522, 3374, 161, 10, 5303, 1498, 2738, 982, 52...</td>\n",
       "      <td>[2979, 3531, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>[what, year, did, kanye, west, open, his, good...</td>\n",
       "      <td>[2004]</td>\n",
       "      <td>[5293, 5385, 1498, 2738, 5291, 3402, 2381, 220...</td>\n",
       "      <td>[237, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>[what, other, artist, besides, kanye, west, an...</td>\n",
       "      <td>[john, legend]</td>\n",
       "      <td>[5293, 3448, 518, 691, 2738, 5291, 438, 1104, ...</td>\n",
       "      <td>[2103, 2235, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>[who, did, kanye, name, president, of, good, m...</td>\n",
       "      <td>[pusha, t]</td>\n",
       "      <td>[5303, 1498, 2738, 3241, 3758, 3374, 2205, 322...</td>\n",
       "      <td>[2979, 3531, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>[what, was, the, goal, of, kanye, ', s, new, c...</td>\n",
       "      <td>[to, make, products, and, experiences, that, p...</td>\n",
       "      <td>[5293, 5253, 4887, 2194, 3374, 2738, 5, 4246, ...</td>\n",
       "      <td>[3640, 2345, 2930, 531, 1457, 3596, 2793, 3845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>[kanye, ', s, creative, content, company, dond...</td>\n",
       "      <td>[mother, donda, west]</td>\n",
       "      <td>[2738, 5, 4246, 1302, 1222, 1118, 1589, 5253, ...</td>\n",
       "      <td>[2520, 1278, 3880, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>[on, what, date, did, kanye, go, public, with,...</td>\n",
       "      <td>[january, 5, ,, 2012]</td>\n",
       "      <td>[3396, 5293, 1368, 1498, 2738, 2193, 3856, 533...</td>\n",
       "      <td>[2079, 326, 11, 246, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>[what, platform, was, kanye, named, a, co, -, ...</td>\n",
       "      <td>[tidal]</td>\n",
       "      <td>[5293, 3657, 5253, 2738, 3242, 225, 1051, 11, ...</td>\n",
       "      <td>[3629, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>[what, longtime, friend, of, kanye, acquired, ...</td>\n",
       "      <td>[jay, -, z]</td>\n",
       "      <td>[5293, 2944, 2083, 3374, 2738, 277, 4939, 2510...</td>\n",
       "      <td>[2084, 12, 3983, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>[what, criticisms, of, other, streaming, platf...</td>\n",
       "      <td>[low, payout, of, royalties]</td>\n",
       "      <td>[5293, 1317, 3374, 3448, 4679, 3658, 1576, 493...</td>\n",
       "      <td>[2300, 2785, 2673, 3185, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>[what, music, streaming, service, is, kanye, w...</td>\n",
       "      <td>[tidal]</td>\n",
       "      <td>[5293, 3225, 4679, 4379, 2651, 2738, 5291, 225...</td>\n",
       "      <td>[3629, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>[what, is, tidal, ', s, specialization, ?]</td>\n",
       "      <td>[lossless, audio, and, high, definition, music...</td>\n",
       "      <td>[5293, 2651, 4939, 5, 4246, 4579, 222]</td>\n",
       "      <td>[2296, 610, 531, 1866, 1183, 2544, 3811, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>[which, famous, rapper, bought, aspiro, ?]</td>\n",
       "      <td>[jay, z]</td>\n",
       "      <td>[5299, 1901, 3949, 762, 531, 222]</td>\n",
       "      <td>[2084, 3983, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>[what, music, service, is, a, huge, competitor...</td>\n",
       "      <td>[spotify]</td>\n",
       "      <td>[5293, 3225, 4379, 2651, 225, 2447, 1133, 2025...</td>\n",
       "      <td>[3429, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>[with, the, help, of, his, mom, ,, what, found...</td>\n",
       "      <td>[kanye, west, foundation]</td>\n",
       "      <td>[5335, 4887, 2346, 3374, 2381, 3172, 10, 5293,...</td>\n",
       "      <td>[2128, 3880, 1598, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>[what, is, the, goal, of, the, kanye, west, fo...</td>\n",
       "      <td>[battle, dropout, and, illiteracy, rates, ,, w...</td>\n",
       "      <td>[5293, 2651, 4887, 2194, 3374, 4887, 2738, 529...</td>\n",
       "      <td>[670, 1298, 531, 1957, 3038, 11, 3888, 2772, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>[what, was, founded, by, kanye, west, and, his...</td>\n",
       "      <td>[kanye, west, foundation]</td>\n",
       "      <td>[5293, 5253, 2055, 829, 2738, 5291, 438, 2381,...</td>\n",
       "      <td>[2128, 3880, 1598, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>[in, what, year, did, the, kanye, west, founda...</td>\n",
       "      <td>[2007]</td>\n",
       "      <td>[2510, 5293, 5385, 1498, 4887, 2738, 5291, 205...</td>\n",
       "      <td>[240, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>[where, was, the, \", kanye, west, foundation, ...</td>\n",
       "      <td>[chicago]</td>\n",
       "      <td>[5297, 5253, 4887, 0, 2738, 5291, 2054, 0, 205...</td>\n",
       "      <td>[925, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>[what, other, mission, besides, dropout, and, ...</td>\n",
       "      <td>[music, education]</td>\n",
       "      <td>[5293, 3448, 3159, 691, 1619, 438, 2483, 3956,...</td>\n",
       "      <td>[2544, 1346, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>[what, campaign, did, the, kanye, west, founda...</td>\n",
       "      <td>[\", ed, in, ', 08, \"]</td>\n",
       "      <td>[5293, 845, 1498, 4887, 2738, 5291, 2054, 3526...</td>\n",
       "      <td>[0, 1342, 1973, 4, 25, 0, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>[in, what, month, was, the, inaugural, concert...</td>\n",
       "      <td>[august]</td>\n",
       "      <td>[2510, 5293, 3184, 5253, 4887, 2512, 1163, 234...</td>\n",
       "      <td>[612, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>[what, was, the, kanye, west, foundation, rena...</td>\n",
       "      <td>[the, dr, ., donda, west, foundation]</td>\n",
       "      <td>[5293, 5253, 4887, 2738, 5291, 2054, 4086, 495...</td>\n",
       "      <td>[3597, 1288, 13, 1278, 3880, 1598, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>[what, year, did, the, foundation, end, its, r...</td>\n",
       "      <td>[2011]</td>\n",
       "      <td>[5293, 5385, 1498, 4887, 2054, 1734, 2666, 424...</td>\n",
       "      <td>[245, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>[in, what, year, did, kanye, west, ', s, mothe...</td>\n",
       "      <td>[2008]</td>\n",
       "      <td>[2510, 5293, 5385, 1498, 2738, 5291, 5, 4246, ...</td>\n",
       "      <td>[241, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>[kanye, ', s, foundation, was, changed, to, wh...</td>\n",
       "      <td>[the, dr, ., donda, west, foundation]</td>\n",
       "      <td>[2738, 5, 4246, 2054, 5253, 935, 4954, 5293, 3...</td>\n",
       "      <td>[3597, 1288, 13, 1278, 3880, 1598, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>[what, year, did, the, foundation, stop, opera...</td>\n",
       "      <td>[2008]</td>\n",
       "      <td>[5293, 5385, 1498, 4887, 2054, 4666, 3411, 222]</td>\n",
       "      <td>[241, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>[what, are, some, charitable, efforts, kanye, ...</td>\n",
       "      <td>[100, black, men, of, america, ,, a, live, ear...</td>\n",
       "      <td>[5293, 498, 4535, 948, 1682, 2738, 5291, 2311,...</td>\n",
       "      <td>[30, 722, 2413, 2673, 518, 11, 420, 2277, 1333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>[during, what, show, did, kanye, take, an, opp...</td>\n",
       "      <td>[a, concert, for, hurricane, relief]</td>\n",
       "      <td>[1638, 5293, 4436, 1498, 2738, 4806, 434, 3417...</td>\n",
       "      <td>[420, 1029, 1580, 1932, 3086, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>[which, u, ., s, ., president, did, kanye, cri...</td>\n",
       "      <td>[george, w, ., bush]</td>\n",
       "      <td>[5299, 5079, 12, 4246, 12, 3758, 1498, 2738, 1...</td>\n",
       "      <td>[1693, 3829, 13, 815, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>[which, president, did, kanye, west, accuse, o...</td>\n",
       "      <td>[george, w, ., bush]</td>\n",
       "      <td>[5299, 3758, 1498, 2738, 5291, 267, 3374, 0, 3...</td>\n",
       "      <td>[1693, 3829, 13, 815, 417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[on, what, day, did, the, kanye, ', s, famous,...</td>\n",
       "      <td>[september, 2, ,, 2005]</td>\n",
       "      <td>[3396, 5293, 1372, 1498, 4887, 2738, 5, 4246, ...</td>\n",
       "      <td>[3286, 229, 11, 238, 417]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "4950  [what, was, the, name, of, west, ', s, fashion...   \n",
       "4951  [the, fashion, line, shown, in, paris, receive...   \n",
       "4952  [on, what, day, did, west, release, his, secon...   \n",
       "4953  [what, brand, struck, a, deal, with, kanye, an...   \n",
       "4954  [how, many, \", seasons, \", of, clothing, did, ...   \n",
       "4955  [what, were, the, shoes, designed, by, kanye, ...   \n",
       "4956  [how, many, pairs, of, shoes, were, sold, in, ...   \n",
       "4957  [what, shoe, was, announced, on, twitter, by, ...   \n",
       "4958  [in, what, year, did, kanye, premier, his, sea...   \n",
       "4959  [what, album, release, coincided, with, kanye,...   \n",
       "4960  [what, restaurant, chain, did, kanye, aspire, ...   \n",
       "4961      [how, many, restaurants, did, kanye, open, ?]   \n",
       "4962  [what, happened, to, the, fatburger, chains, t...   \n",
       "4963  [what, was, the, name, of, the, restaurant, ka...   \n",
       "4964  [how, many, of, kanye, ', s, fatburger, restau...   \n",
       "4965  [when, was, kanye, ', s, last, fatburger, rest...   \n",
       "4966  [what, is, the, name, of, kanye, west, ', s, f...   \n",
       "4967  [what, did, kanye, call, the, label, he, found...   \n",
       "4968  [as, of, 2015, ,, who, did, kanye, choose, as,...   \n",
       "4969  [what, year, did, kanye, west, open, his, good...   \n",
       "4970  [what, other, artist, besides, kanye, west, an...   \n",
       "4971  [who, did, kanye, name, president, of, good, m...   \n",
       "4972  [what, was, the, goal, of, kanye, ', s, new, c...   \n",
       "4973  [kanye, ', s, creative, content, company, dond...   \n",
       "4974  [on, what, date, did, kanye, go, public, with,...   \n",
       "4975  [what, platform, was, kanye, named, a, co, -, ...   \n",
       "4976  [what, longtime, friend, of, kanye, acquired, ...   \n",
       "4977  [what, criticisms, of, other, streaming, platf...   \n",
       "4978  [what, music, streaming, service, is, kanye, w...   \n",
       "4979         [what, is, tidal, ', s, specialization, ?]   \n",
       "4980         [which, famous, rapper, bought, aspiro, ?]   \n",
       "4981  [what, music, service, is, a, huge, competitor...   \n",
       "4982  [with, the, help, of, his, mom, ,, what, found...   \n",
       "4983  [what, is, the, goal, of, the, kanye, west, fo...   \n",
       "4984  [what, was, founded, by, kanye, west, and, his...   \n",
       "4985  [in, what, year, did, the, kanye, west, founda...   \n",
       "4986  [where, was, the, \", kanye, west, foundation, ...   \n",
       "4987  [what, other, mission, besides, dropout, and, ...   \n",
       "4988  [what, campaign, did, the, kanye, west, founda...   \n",
       "4989  [in, what, month, was, the, inaugural, concert...   \n",
       "4990  [what, was, the, kanye, west, foundation, rena...   \n",
       "4991  [what, year, did, the, foundation, end, its, r...   \n",
       "4992  [in, what, year, did, kanye, west, ', s, mothe...   \n",
       "4993  [kanye, ', s, foundation, was, changed, to, wh...   \n",
       "4994  [what, year, did, the, foundation, stop, opera...   \n",
       "4995  [what, are, some, charitable, efforts, kanye, ...   \n",
       "4996  [during, what, show, did, kanye, take, an, opp...   \n",
       "4997  [which, u, ., s, ., president, did, kanye, cri...   \n",
       "4998  [which, president, did, kanye, west, accuse, o...   \n",
       "4999  [on, what, day, did, the, kanye, ', s, famous,...   \n",
       "\n",
       "                                                Answers  \\\n",
       "4950                                  [dw, kanye, west]   \n",
       "4951                        [mixed, -, to, -, negative]   \n",
       "4952                                [march, 6, ,, 2012]   \n",
       "4953                                           [adidas]   \n",
       "4954                                                [3]   \n",
       "4955                            [adidas, yeezy, boosts]   \n",
       "4956                                             [9000]   \n",
       "4957                            [adidas, yeezy, boosts]   \n",
       "4958                                             [2015]   \n",
       "4959                             [the, life, of, pablo]   \n",
       "4960                                        [fatburger]   \n",
       "4961                                                [2]   \n",
       "4962                                       [shut, down]   \n",
       "4963                                        [fatburger]   \n",
       "4964                                                [2]   \n",
       "4965                                             [2011]   \n",
       "4966                                   [kw, foods, llc]   \n",
       "4967                                      [good, music]   \n",
       "4968                                         [pusha, t]   \n",
       "4969                                             [2004]   \n",
       "4970                                     [john, legend]   \n",
       "4971                                         [pusha, t]   \n",
       "4972  [to, make, products, and, experiences, that, p...   \n",
       "4973                              [mother, donda, west]   \n",
       "4974                              [january, 5, ,, 2012]   \n",
       "4975                                            [tidal]   \n",
       "4976                                        [jay, -, z]   \n",
       "4977                       [low, payout, of, royalties]   \n",
       "4978                                            [tidal]   \n",
       "4979  [lossless, audio, and, high, definition, music...   \n",
       "4980                                           [jay, z]   \n",
       "4981                                          [spotify]   \n",
       "4982                          [kanye, west, foundation]   \n",
       "4983  [battle, dropout, and, illiteracy, rates, ,, w...   \n",
       "4984                          [kanye, west, foundation]   \n",
       "4985                                             [2007]   \n",
       "4986                                          [chicago]   \n",
       "4987                                 [music, education]   \n",
       "4988                              [\", ed, in, ', 08, \"]   \n",
       "4989                                           [august]   \n",
       "4990              [the, dr, ., donda, west, foundation]   \n",
       "4991                                             [2011]   \n",
       "4992                                             [2008]   \n",
       "4993              [the, dr, ., donda, west, foundation]   \n",
       "4994                                             [2008]   \n",
       "4995  [100, black, men, of, america, ,, a, live, ear...   \n",
       "4996               [a, concert, for, hurricane, relief]   \n",
       "4997                               [george, w, ., bush]   \n",
       "4998                               [george, w, ., bush]   \n",
       "4999                            [september, 2, ,, 2005]   \n",
       "\n",
       "                                       Question_encoded  \\\n",
       "4950  [5293, 5253, 4887, 3241, 3374, 5291, 5, 4246, ...   \n",
       "4951  [4887, 1910, 2904, 4440, 2510, 3508, 3984, 529...   \n",
       "4952  [3396, 5293, 1372, 1498, 5291, 4063, 2381, 433...   \n",
       "4953  [5293, 773, 4696, 225, 1382, 5335, 2738, 438, ...   \n",
       "4954  [2441, 3029, 0, 4332, 0, 3374, 1047, 1498, 273...   \n",
       "4955  [5293, 5290, 4887, 4425, 1463, 829, 2738, 438,...   \n",
       "4956  [2441, 3029, 3490, 3374, 4425, 5290, 4526, 251...   \n",
       "4957  [5293, 4424, 5253, 451, 3396, 5069, 829, 2738,...   \n",
       "4958  [2510, 5293, 5385, 1498, 2738, 3748, 2381, 433...   \n",
       "4959  [5293, 381, 4063, 1059, 5335, 2738, 5, 4246, 5...   \n",
       "4960  [5293, 4137, 923, 1498, 2738, 530, 4954, 3402,...   \n",
       "4961          [2441, 3029, 4138, 1498, 2738, 3402, 222]   \n",
       "4962  [5293, 2298, 4954, 4887, 1913, 924, 4886, 2738...   \n",
       "4963  [5293, 5253, 4887, 3241, 3374, 4887, 4137, 273...   \n",
       "4964  [2441, 3029, 3374, 2738, 5, 4246, 1913, 4138, ...   \n",
       "4965  [5296, 5253, 2738, 5, 4246, 2820, 1913, 4137, ...   \n",
       "4966  [5293, 2651, 4887, 3241, 3374, 2738, 5291, 5, ...   \n",
       "4967  [5293, 1498, 2738, 837, 4887, 2785, 2316, 2055...   \n",
       "4968  [522, 3374, 161, 10, 5303, 1498, 2738, 982, 52...   \n",
       "4969  [5293, 5385, 1498, 2738, 5291, 3402, 2381, 220...   \n",
       "4970  [5293, 3448, 518, 691, 2738, 5291, 438, 1104, ...   \n",
       "4971  [5303, 1498, 2738, 3241, 3758, 3374, 2205, 322...   \n",
       "4972  [5293, 5253, 4887, 2194, 3374, 2738, 5, 4246, ...   \n",
       "4973  [2738, 5, 4246, 1302, 1222, 1118, 1589, 5253, ...   \n",
       "4974  [3396, 5293, 1368, 1498, 2738, 2193, 3856, 533...   \n",
       "4975  [5293, 3657, 5253, 2738, 3242, 225, 1051, 11, ...   \n",
       "4976  [5293, 2944, 2083, 3374, 2738, 277, 4939, 2510...   \n",
       "4977  [5293, 1317, 3374, 3448, 4679, 3658, 1576, 493...   \n",
       "4978  [5293, 3225, 4679, 4379, 2651, 2738, 5291, 225...   \n",
       "4979             [5293, 2651, 4939, 5, 4246, 4579, 222]   \n",
       "4980                  [5299, 1901, 3949, 762, 531, 222]   \n",
       "4981  [5293, 3225, 4379, 2651, 225, 2447, 1133, 2025...   \n",
       "4982  [5335, 4887, 2346, 3374, 2381, 3172, 10, 5293,...   \n",
       "4983  [5293, 2651, 4887, 2194, 3374, 4887, 2738, 529...   \n",
       "4984  [5293, 5253, 2055, 829, 2738, 5291, 438, 2381,...   \n",
       "4985  [2510, 5293, 5385, 1498, 4887, 2738, 5291, 205...   \n",
       "4986  [5297, 5253, 4887, 0, 2738, 5291, 2054, 0, 205...   \n",
       "4987  [5293, 3448, 3159, 691, 1619, 438, 2483, 3956,...   \n",
       "4988  [5293, 845, 1498, 4887, 2738, 5291, 2054, 3526...   \n",
       "4989  [2510, 5293, 3184, 5253, 4887, 2512, 1163, 234...   \n",
       "4990  [5293, 5253, 4887, 2738, 5291, 2054, 4086, 495...   \n",
       "4991  [5293, 5385, 1498, 4887, 2054, 1734, 2666, 424...   \n",
       "4992  [2510, 5293, 5385, 1498, 2738, 5291, 5, 4246, ...   \n",
       "4993  [2738, 5, 4246, 2054, 5253, 935, 4954, 5293, 3...   \n",
       "4994    [5293, 5385, 1498, 4887, 2054, 4666, 3411, 222]   \n",
       "4995  [5293, 498, 4535, 948, 1682, 2738, 5291, 2311,...   \n",
       "4996  [1638, 5293, 4436, 1498, 2738, 4806, 434, 3417...   \n",
       "4997  [5299, 5079, 12, 4246, 12, 3758, 1498, 2738, 1...   \n",
       "4998  [5299, 3758, 1498, 2738, 5291, 267, 3374, 0, 3...   \n",
       "4999  [3396, 5293, 1372, 1498, 4887, 2738, 5, 4246, ...   \n",
       "\n",
       "                                        Answers_encoded  \n",
       "4950                            [1314, 2128, 3880, 417]  \n",
       "4951                    [2485, 12, 3640, 12, 2581, 417]  \n",
       "4952                          [2363, 350, 11, 246, 417]  \n",
       "4953                                         [449, 417]  \n",
       "4954                                         [280, 417]  \n",
       "4955                              [449, 3971, 751, 417]  \n",
       "4956                                         [402, 417]  \n",
       "4957                              [449, 3971, 751, 417]  \n",
       "4958                                         [249, 417]  \n",
       "4959                      [3597, 2253, 2673, 2741, 417]  \n",
       "4960                                        [1493, 417]  \n",
       "4961                                         [229, 417]  \n",
       "4962                                  [3342, 1286, 417]  \n",
       "4963                                        [1493, 417]  \n",
       "4964                                         [229, 417]  \n",
       "4965                                         [245, 417]  \n",
       "4966                            [2190, 1575, 2281, 417]  \n",
       "4967                                  [1739, 2544, 417]  \n",
       "4968                                  [2979, 3531, 417]  \n",
       "4969                                         [237, 417]  \n",
       "4970                                  [2103, 2235, 417]  \n",
       "4971                                  [2979, 3531, 417]  \n",
       "4972  [3640, 2345, 2930, 531, 1457, 3596, 2793, 3845...  \n",
       "4973                            [2520, 1278, 3880, 417]  \n",
       "4974                          [2079, 326, 11, 246, 417]  \n",
       "4975                                        [3629, 417]  \n",
       "4976                              [2084, 12, 3983, 417]  \n",
       "4977                      [2300, 2785, 2673, 3185, 417]  \n",
       "4978                                        [3629, 417]  \n",
       "4979      [2296, 610, 531, 1866, 1183, 2544, 3811, 417]  \n",
       "4980                                  [2084, 3983, 417]  \n",
       "4981                                        [3429, 417]  \n",
       "4982                            [2128, 3880, 1598, 417]  \n",
       "4983  [670, 1298, 531, 1957, 3038, 11, 3888, 2772, 3...  \n",
       "4984                            [2128, 3880, 1598, 417]  \n",
       "4985                                         [240, 417]  \n",
       "4986                                         [925, 417]  \n",
       "4987                                  [2544, 1346, 417]  \n",
       "4988                     [0, 1342, 1973, 4, 25, 0, 417]  \n",
       "4989                                         [612, 417]  \n",
       "4990            [3597, 1288, 13, 1278, 3880, 1598, 417]  \n",
       "4991                                         [245, 417]  \n",
       "4992                                         [241, 417]  \n",
       "4993            [3597, 1288, 13, 1278, 3880, 1598, 417]  \n",
       "4994                                         [241, 417]  \n",
       "4995  [30, 722, 2413, 2673, 518, 11, 420, 2277, 1333...  \n",
       "4996                 [420, 1029, 1580, 1932, 3086, 417]  \n",
       "4997                         [1693, 3829, 13, 815, 417]  \n",
       "4998                         [1693, 3829, 13, 815, 417]  \n",
       "4999                          [3286, 229, 11, 238, 417]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check on Question and Answer Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08612397  0.4683421   1.1202817   0.89838433 -0.02509592 -0.59979767\n",
      "  1.5051852   0.72202337 -0.48813638  0.24514244  0.4102122   0.0113057\n",
      "  0.5960555  -0.45577106  0.14036338  0.21082228 -1.4051661  -0.15541624\n",
      " -1.0806677  -0.8043154   0.07190805 -0.16877684  0.3798714  -0.37297195\n",
      " -2.3234746  -0.04577581  0.57446414 -0.18885757 -1.1524972   0.47721928\n",
      "  0.17444377 -0.31108376  0.2087786   0.70430166  0.42223585  0.41096598\n",
      "  0.70646995  0.24673785 -0.84970194  0.5304875   0.08908914 -0.58689964\n",
      " -0.4281494   0.71896255  1.9707432  -0.2528021  -0.08564414 -0.0645953\n",
      "  0.9101291  -1.0712231   0.22804962  0.50006104  0.33930728 -0.13008718\n",
      " -1.3227125   0.20961478  0.39179873  0.84373707  0.30207354 -0.3978397\n",
      "  0.662881    0.6367943  -1.0932344  -1.1294899   0.26111436  0.40094912\n",
      " -0.16343357  1.5931644  -0.26111376 -0.3025634  -0.04134805 -0.36030748\n",
      " -0.04972831  0.71029353 -0.52045906  0.856472    0.07366011  0.3339381\n",
      "  0.12269638 -0.8888106  -0.79770845  0.5684257  -0.5015197  -0.14041203\n",
      " -0.68784827 -0.03875216  0.18263218  0.42732847  0.4749071  -0.05227673\n",
      " -0.18581495  0.10985609 -0.4447832   0.568123    0.6523084  -0.20954296\n",
      " -0.3988236  -1.217424    0.32778555 -0.261152  ]\n",
      "[ 0.08612397  0.4683421   1.1202817   0.89838433 -0.02509592 -0.59979767\n",
      "  1.5051852   0.72202337 -0.48813638  0.24514244  0.4102122   0.0113057\n",
      "  0.5960555  -0.45577106  0.14036338  0.21082228 -1.4051661  -0.15541624\n",
      " -1.0806677  -0.8043154   0.07190805 -0.16877684  0.3798714  -0.37297195\n",
      " -2.3234746  -0.04577581  0.57446414 -0.18885757 -1.1524972   0.47721928\n",
      "  0.17444377 -0.31108376  0.2087786   0.70430166  0.42223585  0.41096598\n",
      "  0.70646995  0.24673785 -0.84970194  0.5304875   0.08908914 -0.58689964\n",
      " -0.4281494   0.71896255  1.9707432  -0.2528021  -0.08564414 -0.0645953\n",
      "  0.9101291  -1.0712231   0.22804962  0.50006104  0.33930728 -0.13008718\n",
      " -1.3227125   0.20961478  0.39179873  0.84373707  0.30207354 -0.3978397\n",
      "  0.662881    0.6367943  -1.0932344  -1.1294899   0.26111436  0.40094912\n",
      " -0.16343357  1.5931644  -0.26111376 -0.3025634  -0.04134805 -0.36030748\n",
      " -0.04972831  0.71029353 -0.52045906  0.856472    0.07366011  0.3339381\n",
      "  0.12269638 -0.8888106  -0.79770845  0.5684257  -0.5015197  -0.14041203\n",
      " -0.68784827 -0.03875216  0.18263218  0.42732847  0.4749071  -0.05227673\n",
      " -0.18581495  0.10985609 -0.4447832   0.568123    0.6523084  -0.20954296\n",
      " -0.3988236  -1.217424    0.32778555 -0.261152  ]\n"
     ]
    }
   ],
   "source": [
    "# the embedding weights are the same for both question and answer. \n",
    "# This is expected as the same word \"in\" is looked up in the Q and A vector representation\n",
    "\n",
    "print(Q_embeddings[Q_vocab.word2index['in']])\n",
    "print(A_embeddings[A_vocab.word2index['in']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert list of indices in Question and Answers to torch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data_df['Question_encoded'].tolist()\n",
    "\n",
    "TRG = data_df['Answers_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Encoder\n",
    "\n",
    "* A Seq2Seq architecture consists of an encoder and a decoder unit. You will use Pytorch to build a full Seq2Seq model.\n",
    "\n",
    "* The first step of the architecture is to create an encoder with an LSTM unit.\n",
    "\n",
    "(Extra Credit)\n",
    "\n",
    "* Load your pretrained embeddings into the LSTM unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_embeddings = torch.Tensor(Q_embeddings)\n",
    "A_embeddings = torch.Tensor(A_embeddings)\n",
    "\n",
    "_, Q_embedding_dim = Q_embeddings.shape\n",
    "_, A_embedding_dim = A_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, pretrained_embeddings=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "            \n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        return x, hidden, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Decoder\n",
    "\n",
    "* The second step of the architecture is to create a decoder using a second LSTM unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, pretrained_embeddings=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # self.embedding is a vector representation of the target to our model\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "            \n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.fc(x[0])\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x, hidden, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine them into a Seq2Seq Architecture\n",
    "\n",
    "* To finalize your model, you will combine the encoder and decoder units into a working model.\n",
    "\n",
    "* The Seq2Seq2 model must be able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, Q_embeddings=None, A_embeddings=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size, Q_embeddings)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size, A_embeddings)\n",
    "        \n",
    "    def forward(self, src, trg, src_len, trg_len, teacher_force=1, top_k = 1):\n",
    "        \n",
    "        output = {\n",
    "            'decoder_output':[]\n",
    "        }\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device)\n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)  \n",
    "        \n",
    "        for i in range(src_len):\n",
    "            encoder_output, encoder_hidden, cell_state = self.encoder(src[i], encoder_hidden, cell_state)\n",
    "\n",
    "        decoder_input = torch.Tensor([[0]]).long().to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            decoder_output, decoder_hidden, cell_state = self.decoder(decoder_input, decoder_hidden, cell_state)\n",
    "            output['decoder_output'].append(decoder_output)\n",
    "            \n",
    "            if self.training:\n",
    "                decoder_input = trg[i] if random.random() > teacher_force else decoder_output.argmax(1)\n",
    "            else:\n",
    "                _, top_index = decoder_output.data.topk(top_k)\n",
    "                decoder_input = top_index.squeeze().detach()\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train & evaluate your model\n",
    "\n",
    "* Finally you will train and evaluate your model using a Pytorch training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "    \n",
    "def get_train_val_indices(data, total_epochs, random_seed=1):\n",
    "    \n",
    "    # Create the KFold object with the random seed \n",
    "    kf = KFold(n_splits=total_epochs, shuffle=True, random_state=random_seed)\n",
    "    # Get all train and val indices at once\n",
    "    all_train_indices, all_val_indices = zip(*kf.split(data))\n",
    "    \n",
    "    return all_train_indices, all_val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def train(source_data, target_data, output_size, model, epochs, print_every, learning_rate, device, top_k=1):\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    loss = 0\n",
    "    \n",
    "\n",
    "    # Use KFold to obtain the train and val indices for all epochs all_train_indices, all_val_indices\n",
    "    train_indices, val_indices = get_train_val_indices(source_data, epochs)\n",
    "    \n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        # train set\n",
    "        model.train()\n",
    "        \n",
    "        current_train_epoch_loss = 0\n",
    "        current_val_epoch_loss = 0\n",
    "        \n",
    "        train_size = len(train_indices[epoch])\n",
    "        for i in range(train_size):\n",
    "            train_index = train_indices[epoch][i]\n",
    "            \n",
    "            src = source_data[train_index]\n",
    "            trg = target_data[train_index]\n",
    "            \n",
    "            output = model(\n",
    "                src = src, \n",
    "                trg = trg, \n",
    "                src_len = src.size(0), \n",
    "                trg_len = trg.size(0), \n",
    "                top_k = top_k)\n",
    "            \n",
    "            output = torch.stack(output[\"decoder_output\"]).squeeze()\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            current_train_epoch_loss += loss.item()  # Accumulate the loss item-wise\n",
    "        \n",
    "        current_train_epoch_loss /= train_size  # Calculate average loss per batch\n",
    "        total_train_loss += current_train_epoch_loss\n",
    "    \n",
    "        # validation set \n",
    "        model.eval()\n",
    "        \n",
    "        val_size = len(val_indices[epoch])\n",
    "        for j in range(val_size):\n",
    "            val_index = val_indices[epoch][j]\n",
    "            \n",
    "            src = source_data[val_index]\n",
    "            trg = target_data[val_index]\n",
    "\n",
    "\n",
    "            output = model(\n",
    "                src = src, \n",
    "                trg = trg, \n",
    "                src_len = src.size(0), \n",
    "                trg_len = trg.size(0), \n",
    "                top_k = top_k)\n",
    "            \n",
    "            output = torch.stack(output[\"decoder_output\"]).squeeze()\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            current_val_epoch_loss += loss.item()  # Accumulate the loss item-wise\n",
    "        \n",
    "        current_val_epoch_loss /= val_size  # Calculate average loss per batch\n",
    "        total_val_loss += current_val_epoch_loss\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            train_loss_average = total_train_loss / (print_every)\n",
    "            val_loss_average = total_val_loss / (print_every)\n",
    "            print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format((epoch + 1), epochs, train_loss_average, val_loss_average))\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "        \n",
    "    del train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(Q_vocab_list)\n",
    "output_size = len(A_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_tensor = [torch.tensor(src_items).to(device) for src_items in SRC]\n",
    "TRG_tensor = [torch.tensor(trg_items).to(device) for trg_items in TRG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without pretrained embeddings\n",
    "seq2seq = Seq2Seq(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/65 Epoch  -  Training Loss = 5.8328  -  Validation Loss = 5.6909\n",
      "2/65 Epoch  -  Training Loss = 5.4950  -  Validation Loss = 5.4832\n",
      "3/65 Epoch  -  Training Loss = 5.3877  -  Validation Loss = 5.3397\n",
      "4/65 Epoch  -  Training Loss = 5.3194  -  Validation Loss = 5.2804\n",
      "5/65 Epoch  -  Training Loss = 5.2691  -  Validation Loss = 5.3494\n",
      "6/65 Epoch  -  Training Loss = 5.2280  -  Validation Loss = 5.3965\n",
      "7/65 Epoch  -  Training Loss = 5.1998  -  Validation Loss = 5.2261\n",
      "8/65 Epoch  -  Training Loss = 5.1717  -  Validation Loss = 5.2898\n",
      "9/65 Epoch  -  Training Loss = 5.1507  -  Validation Loss = 5.1738\n",
      "10/65 Epoch  -  Training Loss = 5.1322  -  Validation Loss = 5.0322\n",
      "11/65 Epoch  -  Training Loss = 5.1067  -  Validation Loss = 5.4198\n",
      "12/65 Epoch  -  Training Loss = 5.0905  -  Validation Loss = 5.2833\n",
      "13/65 Epoch  -  Training Loss = 5.0761  -  Validation Loss = 5.1627\n",
      "14/65 Epoch  -  Training Loss = 5.0591  -  Validation Loss = 5.1888\n",
      "15/65 Epoch  -  Training Loss = 5.0436  -  Validation Loss = 5.0683\n",
      "16/65 Epoch  -  Training Loss = 5.0228  -  Validation Loss = 5.1342\n",
      "17/65 Epoch  -  Training Loss = 4.9985  -  Validation Loss = 5.2085\n",
      "18/65 Epoch  -  Training Loss = 4.9679  -  Validation Loss = 5.2337\n",
      "19/65 Epoch  -  Training Loss = 4.9286  -  Validation Loss = 5.0217\n",
      "20/65 Epoch  -  Training Loss = 4.8811  -  Validation Loss = 4.9873\n",
      "21/65 Epoch  -  Training Loss = 4.8321  -  Validation Loss = 4.9326\n",
      "22/65 Epoch  -  Training Loss = 4.7843  -  Validation Loss = 4.7517\n",
      "23/65 Epoch  -  Training Loss = 4.7291  -  Validation Loss = 4.9837\n",
      "24/65 Epoch  -  Training Loss = 4.6752  -  Validation Loss = 4.8029\n",
      "25/65 Epoch  -  Training Loss = 4.6150  -  Validation Loss = 5.0362\n",
      "26/65 Epoch  -  Training Loss = 4.5570  -  Validation Loss = 4.9007\n",
      "27/65 Epoch  -  Training Loss = 4.4995  -  Validation Loss = 4.9254\n",
      "28/65 Epoch  -  Training Loss = 4.4360  -  Validation Loss = 4.8981\n",
      "29/65 Epoch  -  Training Loss = 4.3809  -  Validation Loss = 4.4798\n",
      "30/65 Epoch  -  Training Loss = 4.3139  -  Validation Loss = 4.7687\n",
      "31/65 Epoch  -  Training Loss = 4.2533  -  Validation Loss = 4.5076\n",
      "32/65 Epoch  -  Training Loss = 4.1906  -  Validation Loss = 4.6522\n",
      "33/65 Epoch  -  Training Loss = 4.1227  -  Validation Loss = 4.6275\n",
      "34/65 Epoch  -  Training Loss = 4.0603  -  Validation Loss = 4.5443\n",
      "35/65 Epoch  -  Training Loss = 3.9967  -  Validation Loss = 4.5080\n",
      "36/65 Epoch  -  Training Loss = 3.9236  -  Validation Loss = 4.4412\n",
      "37/65 Epoch  -  Training Loss = 3.8628  -  Validation Loss = 4.4842\n",
      "38/65 Epoch  -  Training Loss = 3.7979  -  Validation Loss = 4.8579\n",
      "39/65 Epoch  -  Training Loss = 3.7570  -  Validation Loss = 4.3266\n",
      "40/65 Epoch  -  Training Loss = 3.6769  -  Validation Loss = 4.0711\n",
      "41/65 Epoch  -  Training Loss = 3.5928  -  Validation Loss = 4.3999\n",
      "42/65 Epoch  -  Training Loss = 3.5270  -  Validation Loss = 4.4303\n",
      "43/65 Epoch  -  Training Loss = 3.4739  -  Validation Loss = 4.1463\n",
      "44/65 Epoch  -  Training Loss = 3.4216  -  Validation Loss = 4.1636\n",
      "45/65 Epoch  -  Training Loss = 3.3517  -  Validation Loss = 4.1868\n",
      "46/65 Epoch  -  Training Loss = 3.2819  -  Validation Loss = 3.9638\n",
      "47/65 Epoch  -  Training Loss = 3.2069  -  Validation Loss = 4.2772\n",
      "48/65 Epoch  -  Training Loss = 3.1788  -  Validation Loss = 4.1045\n",
      "49/65 Epoch  -  Training Loss = 3.1237  -  Validation Loss = 3.8740\n",
      "50/65 Epoch  -  Training Loss = 3.0334  -  Validation Loss = 3.9090\n",
      "51/65 Epoch  -  Training Loss = 2.9855  -  Validation Loss = 4.1728\n",
      "52/65 Epoch  -  Training Loss = 2.9051  -  Validation Loss = 4.1806\n",
      "53/65 Epoch  -  Training Loss = 2.8207  -  Validation Loss = 4.2055\n",
      "54/65 Epoch  -  Training Loss = 2.7545  -  Validation Loss = 3.7013\n",
      "55/65 Epoch  -  Training Loss = 2.7145  -  Validation Loss = 3.6158\n",
      "56/65 Epoch  -  Training Loss = 2.7017  -  Validation Loss = 3.6959\n",
      "57/65 Epoch  -  Training Loss = 2.6857  -  Validation Loss = 3.9523\n",
      "58/65 Epoch  -  Training Loss = 2.6094  -  Validation Loss = 3.4498\n",
      "59/65 Epoch  -  Training Loss = 2.5237  -  Validation Loss = 3.1958\n",
      "60/65 Epoch  -  Training Loss = 2.4338  -  Validation Loss = 3.4399\n",
      "61/65 Epoch  -  Training Loss = 2.3765  -  Validation Loss = 3.3428\n",
      "62/65 Epoch  -  Training Loss = 2.3154  -  Validation Loss = 3.2785\n",
      "63/65 Epoch  -  Training Loss = 2.2635  -  Validation Loss = 3.2120\n",
      "64/65 Epoch  -  Training Loss = 2.2076  -  Validation Loss = 3.1361\n",
      "65/65 Epoch  -  Training Loss = 2.1477  -  Validation Loss = 3.1773\n"
     ]
    }
   ],
   "source": [
    "train(source_data = SRC_tensor,\n",
    "      target_data = TRG_tensor,\n",
    "      output_size = output_size,\n",
    "      model = seq2seq,\n",
    "      epochs = epochs,\n",
    "      print_every = 1,\n",
    "      learning_rate=learning_rate,\n",
    "      device = device,\n",
    "      top_k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'seq2seq_v1.pt'\n",
    "\n",
    "torch.save(seq2seq, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5437, 100)\n",
       "    (lstm): LSTM(100, 100)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(4020, 100)\n",
       "    (lstm): LSTM(100, 100)\n",
       "    (fc): Linear(in_features=100, out_features=4020, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interact with the Chatbot\n",
    "\n",
    "* Demonstrate your chatbot by converting the outputs of the model to text and displaying it's responses at the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 - without trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(sentence, trg, vocab, model, top_k=5):\n",
    "    \n",
    "    try:\n",
    "        sentence = prepare_text(sentence)\n",
    "        src = encode_Q_sentence(sentence)\n",
    "    except:\n",
    "        print(\"Warning: Word does not exist in vocabulary!\")\n",
    "        return\n",
    "    \n",
    "    answer_words = []\n",
    "    \n",
    "    src = [torch.tensor(s).to(device) for s in src]\n",
    "    src = torch.tensor(src).to(device)\n",
    "\n",
    "    \n",
    "    output = model(src, trg, src.size(0), len(trg))\n",
    "\n",
    "    for o in output['decoder_output']:\n",
    "        \n",
    "        top_v, top_i = o.data.topk(top_k)\n",
    "        top_v = torch.exp(top_v)\n",
    "        sampled_top = torch.multinomial(top_v/top_v.sum(), 1, replacement=True)\n",
    "        top_i = top_i[0][sampled_top.item()]\n",
    "        \n",
    "        if top_i.item() == EOS_INDEX:\n",
    "            # when EOS is reached\n",
    "            break\n",
    "        else:\n",
    "            word = vocab.index2word[top_i.item()]\n",
    "            answer_words.append(word)\n",
    "            \n",
    "    print(\"<\", ' '.join(answer_words), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n",
      "> which decade did beyonce become famous?\n",
      "< late 1990s \n",
      "\n",
      "> when did beyonce become popular?\n",
      "< 1990s 1990s \n",
      "\n",
      "> which artist did beyonce marry?\n",
      "< jay z \n",
      "\n",
      "> when did beyonce take a hiatus?\n",
      "< 21 \n",
      "\n",
      "> what magazine rate beyonce as the most popular?\n",
      "< heat \n",
      "\n",
      "> what race was beyonce's father?\n",
      "< the mother carter \n",
      "\n",
      "> in what year kanye premier?\n",
      "< 2013 \n",
      "\n",
      "> what brand did kanye struck a deal with?\n",
      "< adidas - \n",
      "\n",
      "> the fashion line in paris shown what review?\n",
      "< the and , of \n",
      "\n",
      "> exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    sample(src, TRG_tensor, A_vocab, seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 - with trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pretrained embeddings\n",
    "seq2seq = Seq2Seq(input_size, hidden_size, output_size, Q_embeddings, A_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/65 Epoch  -  Training Loss = 6.0508  -  Validation Loss = 5.7471\n",
      "2/65 Epoch  -  Training Loss = 5.5484  -  Validation Loss = 5.5019\n",
      "3/65 Epoch  -  Training Loss = 5.4314  -  Validation Loss = 5.3690\n",
      "4/65 Epoch  -  Training Loss = 5.3635  -  Validation Loss = 5.3351\n",
      "5/65 Epoch  -  Training Loss = 5.3163  -  Validation Loss = 5.4205\n",
      "6/65 Epoch  -  Training Loss = 5.2785  -  Validation Loss = 5.4462\n",
      "7/65 Epoch  -  Training Loss = 5.2516  -  Validation Loss = 5.2689\n",
      "8/65 Epoch  -  Training Loss = 5.2238  -  Validation Loss = 5.3442\n",
      "9/65 Epoch  -  Training Loss = 5.2025  -  Validation Loss = 5.2245\n",
      "10/65 Epoch  -  Training Loss = 5.1842  -  Validation Loss = 5.0959\n",
      "11/65 Epoch  -  Training Loss = 5.1584  -  Validation Loss = 5.4630\n",
      "12/65 Epoch  -  Training Loss = 5.1409  -  Validation Loss = 5.3569\n",
      "13/65 Epoch  -  Training Loss = 5.1254  -  Validation Loss = 5.1821\n",
      "14/65 Epoch  -  Training Loss = 5.1070  -  Validation Loss = 5.2380\n",
      "15/65 Epoch  -  Training Loss = 5.0913  -  Validation Loss = 5.1232\n",
      "16/65 Epoch  -  Training Loss = 5.0722  -  Validation Loss = 5.2048\n",
      "17/65 Epoch  -  Training Loss = 5.0484  -  Validation Loss = 5.3084\n",
      "18/65 Epoch  -  Training Loss = 5.0192  -  Validation Loss = 5.3233\n",
      "19/65 Epoch  -  Training Loss = 4.9862  -  Validation Loss = 5.3025\n",
      "20/65 Epoch  -  Training Loss = 4.9484  -  Validation Loss = 5.1568\n",
      "21/65 Epoch  -  Training Loss = 4.9046  -  Validation Loss = 5.1419\n",
      "22/65 Epoch  -  Training Loss = 4.8641  -  Validation Loss = 4.8675\n",
      "23/65 Epoch  -  Training Loss = 4.8145  -  Validation Loss = 5.1643\n",
      "24/65 Epoch  -  Training Loss = 4.7775  -  Validation Loss = 5.0468\n",
      "25/65 Epoch  -  Training Loss = 4.7412  -  Validation Loss = 5.2434\n",
      "26/65 Epoch  -  Training Loss = 4.7042  -  Validation Loss = 5.1121\n",
      "27/65 Epoch  -  Training Loss = 4.6594  -  Validation Loss = 5.1204\n",
      "28/65 Epoch  -  Training Loss = 4.6178  -  Validation Loss = 5.2045\n",
      "29/65 Epoch  -  Training Loss = 4.6212  -  Validation Loss = 4.8054\n",
      "30/65 Epoch  -  Training Loss = 4.5706  -  Validation Loss = 5.1530\n",
      "31/65 Epoch  -  Training Loss = 4.5256  -  Validation Loss = 4.8794\n",
      "32/65 Epoch  -  Training Loss = 4.4939  -  Validation Loss = 4.9981\n",
      "33/65 Epoch  -  Training Loss = 4.4543  -  Validation Loss = 4.9518\n",
      "34/65 Epoch  -  Training Loss = 4.4236  -  Validation Loss = 4.9025\n",
      "35/65 Epoch  -  Training Loss = 4.3871  -  Validation Loss = 4.9028\n",
      "36/65 Epoch  -  Training Loss = 4.3404  -  Validation Loss = 4.9421\n",
      "37/65 Epoch  -  Training Loss = 4.3092  -  Validation Loss = 4.9236\n",
      "38/65 Epoch  -  Training Loss = 4.2745  -  Validation Loss = 5.4903\n",
      "39/65 Epoch  -  Training Loss = 4.2515  -  Validation Loss = 4.7304\n",
      "40/65 Epoch  -  Training Loss = 4.2161  -  Validation Loss = 4.7178\n",
      "41/65 Epoch  -  Training Loss = 4.1768  -  Validation Loss = 5.0255\n",
      "42/65 Epoch  -  Training Loss = 4.1430  -  Validation Loss = 5.0711\n",
      "43/65 Epoch  -  Training Loss = 4.1130  -  Validation Loss = 4.7937\n",
      "44/65 Epoch  -  Training Loss = 4.0838  -  Validation Loss = 4.6433\n",
      "45/65 Epoch  -  Training Loss = 4.0489  -  Validation Loss = 5.0040\n",
      "46/65 Epoch  -  Training Loss = 4.0211  -  Validation Loss = 4.7934\n",
      "47/65 Epoch  -  Training Loss = 3.9811  -  Validation Loss = 5.0218\n",
      "48/65 Epoch  -  Training Loss = 3.9507  -  Validation Loss = 4.6964\n",
      "49/65 Epoch  -  Training Loss = 3.9262  -  Validation Loss = 4.8115\n",
      "50/65 Epoch  -  Training Loss = 3.8884  -  Validation Loss = 4.8134\n",
      "51/65 Epoch  -  Training Loss = 3.8565  -  Validation Loss = 5.0068\n",
      "52/65 Epoch  -  Training Loss = 3.8394  -  Validation Loss = 4.9951\n",
      "53/65 Epoch  -  Training Loss = 3.8062  -  Validation Loss = 5.0239\n",
      "54/65 Epoch  -  Training Loss = 3.7856  -  Validation Loss = 5.0065\n",
      "55/65 Epoch  -  Training Loss = 3.7588  -  Validation Loss = 4.7472\n",
      "56/65 Epoch  -  Training Loss = 3.7290  -  Validation Loss = 4.8013\n",
      "57/65 Epoch  -  Training Loss = 3.7402  -  Validation Loss = 5.1701\n",
      "58/65 Epoch  -  Training Loss = 3.7288  -  Validation Loss = 4.6700\n",
      "59/65 Epoch  -  Training Loss = 3.6934  -  Validation Loss = 4.6564\n",
      "60/65 Epoch  -  Training Loss = 3.6710  -  Validation Loss = 4.9597\n",
      "61/65 Epoch  -  Training Loss = 3.6407  -  Validation Loss = 4.7823\n",
      "62/65 Epoch  -  Training Loss = 3.6145  -  Validation Loss = 4.8662\n",
      "63/65 Epoch  -  Training Loss = 3.5976  -  Validation Loss = 4.5032\n",
      "64/65 Epoch  -  Training Loss = 3.5718  -  Validation Loss = 4.6447\n",
      "65/65 Epoch  -  Training Loss = 3.5429  -  Validation Loss = 4.7321\n"
     ]
    }
   ],
   "source": [
    "train(source_data = SRC_tensor,\n",
    "      target_data = TRG_tensor,\n",
    "      output_size = output_size,\n",
    "      model = seq2seq,\n",
    "      epochs = epochs,\n",
    "      print_every = 1,\n",
    "      learning_rate=learning_rate,\n",
    "      device = device,\n",
    "      top_k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'seq2seq_v2.pt'\n",
    "\n",
    "torch.save(seq2seq, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n",
      "> which decade did beyonce become famous?\n",
      "< graduation - \n",
      "\n",
      "> when did beyonce become popular?\n",
      "< september 2015 , 2011 \n",
      "\n",
      "> which artist did beyonce marry?\n",
      "< jay z \n",
      "\n",
      "> when did beyonce take a hiatus?\n",
      "< january 2005 \n",
      "\n",
      "> what magazine rate beyonce as the most popular?\n",
      "< yeezus - of \n",
      "\n",
      "> what race was beyonce's father?\n",
      "< paris \n",
      "\n",
      "> in what year kanye premier?\n",
      "< 1825 \n",
      "\n",
      "> what brand did kanye struck a deal with?\n",
      "< adidas \n",
      "\n",
      "> the fashion line in paris shown what review?\n",
      "< coachella \n",
      "\n",
      "> exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    sample(src, TRG_tensor, A_vocab, seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
